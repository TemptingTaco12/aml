{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Acknowledgements\n\nThis notebook was originally prepared by Dr. Simone Rossi, who has been a Ph.D. student and then a Postdoctoral Researcher at Eurecom.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import pandas as pd \nimport matplotlib.pyplot as plt \nimport numpy as np \nimport seaborn as sns\nfrom tqdm.notebook import tqdm \n\nimport matplotlib\nfrom matplotlib import rc\n\nrc(\"font\", **{\"family\": \"sans-serif\", \"sans-serif\": \"DejaVu Sans\"})\nrc(\"figure\", **{\"dpi\": 200})\nrc(\n    \"axes\",\n    **{\"spines.right\": False, \"spines.top\": False, \"xmargin\": 0.0, \"ymargin\": 0.05}\n)","metadata":{"execution":{"iopub.status.busy":"2023-04-03T13:01:00.109735Z","iopub.execute_input":"2023-04-03T13:01:00.110192Z","iopub.status.idle":"2023-04-03T13:01:00.840722Z","shell.execute_reply.started":"2023-04-03T13:01:00.110151Z","shell.execute_reply":"2023-04-03T13:01:00.839405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/eurecom-aml-2023-challenge-1/public/train.csv', low_memory=True)","metadata":{"execution":{"iopub.status.busy":"2023-04-03T13:01:05.179758Z","iopub.execute_input":"2023-04-03T13:01:05.180171Z","iopub.status.idle":"2023-04-03T13:02:25.226822Z","shell.execute_reply.started":"2023-04-03T13:01:05.180128Z","shell.execute_reply":"2023-04-03T13:02:25.225779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = pd.read_csv('../input/eurecom-aml-2023-challenge-1/public/test_feat.csv', low_memory=True)","metadata":{"execution":{"iopub.status.busy":"2023-04-03T13:02:45.805051Z","iopub.execute_input":"2023-04-03T13:02:45.805575Z","iopub.status.idle":"2023-04-03T13:02:57.100936Z","shell.execute_reply.started":"2023-04-03T13:02:45.805523Z","shell.execute_reply":"2023-04-03T13:02:57.099896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2023-04-03T13:02:57.102839Z","iopub.execute_input":"2023-04-03T13:02:57.103238Z","iopub.status.idle":"2023-04-03T13:02:57.154003Z","shell.execute_reply.started":"2023-04-03T13:02:57.103202Z","shell.execute_reply":"2023-04-03T13:02:57.152526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data analysis\n\nIn this challenge, you are free (and encouraged) to explore in depth the data you have, you can run simple queries on the data, perform exploration and compute statistics.\n\n**NOTE**: finding the right question to ask is difficult! Don't be afraid to complement your analysis with your own questions. This can give you extra points!\n\n**NOTE 2**: the presentation quality is critical in any business-oriented data analysis. Take time to create few but informative plots, rather than endless tables! ","metadata":{}},{"cell_type":"code","source":"tr_coordinates = df[['fact_latitude','fact_longitude']].drop_duplicates()\nte_coordinates = df_test[['fact_latitude','fact_longitude']].drop_duplicates()","metadata":{"execution":{"iopub.status.busy":"2023-04-03T13:03:09.047719Z","iopub.execute_input":"2023-04-03T13:03:09.048162Z","iopub.status.idle":"2023-04-03T13:03:09.215406Z","shell.execute_reply.started":"2023-04-03T13:03:09.048122Z","shell.execute_reply":"2023-04-03T13:03:09.214162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged = te_coordinates.merge(tr_coordinates, how='left', indicator=True)\ntropic_coordinates = merged[merged['_merge']=='left_only']\ncommon_coordinates = merged[merged['_merge']=='both']","metadata":{"execution":{"iopub.status.busy":"2023-04-03T13:03:13.197822Z","iopub.execute_input":"2023-04-03T13:03:13.198277Z","iopub.status.idle":"2023-04-03T13:03:13.233337Z","shell.execute_reply.started":"2023-04-03T13:03:13.198238Z","shell.execute_reply":"2023-04-03T13:03:13.232381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, (ax0, ax1) = plt.subplots(1, 2, figsize=[10, 3], dpi=200, sharex=True, sharey=True)\nax0.scatter(tr_coordinates['fact_longitude'], tr_coordinates['fact_latitude'], \n            s=1, c='tab:green', label='Train data')\nax1.scatter(common_coordinates['fact_longitude'], common_coordinates['fact_latitude'], \n            s=1, c='tab:orange', label='In-domain test data')\nax1.scatter(tropic_coordinates['fact_longitude'], tropic_coordinates['fact_latitude'], \n            s=1, c='tab:red', label='Out-domain test data')\nax0.legend(), ax1.legend()\nfig.suptitle('Distribution of train and test points in the dataset', y=1.01, fontsize=14)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-03T13:03:16.543564Z","iopub.execute_input":"2023-04-03T13:03:16.544018Z","iopub.status.idle":"2023-04-03T13:03:17.439871Z","shell.execute_reply.started":"2023-04-03T13:03:16.543975Z","shell.execute_reply":"2023-04-03T13:03:17.43888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n# Data Pre-processing\n\nThe previous step should give you a better understanding of which pre-processing is required for the data. This may include:\n\n-    Normalising and standardising the given data;\n-    Removing outliers;\n-    Carrying out feature selection, possibly using metrics derived from information theory;\n-    Handling missing information in the dataset;\n-    Augmenting the dataset with external information;\n-    Combining existing features.\n\nBelow is a very basic example of pre-processing steps.\n","metadata":{}},{"cell_type":"code","source":"col_selection = [\n    'index', 'fact_time', 'fact_latitude', 'fact_longitude',\n    'topography_bathymetry', 'sun_elevation', 'cmc_precipitations', \n    'gfs_a_vorticity', 'gfs_cloudness', 'gfs_clouds_sea', 'gfs_humidity',\n    'fact_temperature'\n]","metadata":{"execution":{"iopub.status.busy":"2023-04-03T13:03:23.143007Z","iopub.execute_input":"2023-04-03T13:03:23.143395Z","iopub.status.idle":"2023-04-03T13:03:23.149815Z","shell.execute_reply.started":"2023-04-03T13:03:23.143363Z","shell.execute_reply":"2023-04-03T13:03:23.148445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df[col_selection].dropna().iloc[:, 1:-1].values\ny = df[col_selection].dropna().iloc[:, -1].values","metadata":{"execution":{"iopub.status.busy":"2023-04-03T13:03:26.591644Z","iopub.execute_input":"2023-04-03T13:03:26.592922Z","iopub.status.idle":"2023-04-03T13:03:27.005694Z","shell.execute_reply.started":"2023-04-03T13:03:26.592855Z","shell.execute_reply":"2023-04-03T13:03:27.004037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Xmean, Xstd, ymean, ystd = X.mean(0), X.std(0), y.mean(), y.std()\nX = (X - Xmean) / Xstd\ny = (y - ymean) / ystd","metadata":{"execution":{"iopub.status.busy":"2023-04-03T13:03:28.909756Z","iopub.execute_input":"2023-04-03T13:03:28.910384Z","iopub.status.idle":"2023-04-03T13:03:29.08194Z","shell.execute_reply.started":"2023-04-03T13:03:28.910329Z","shell.execute_reply":"2023-04-03T13:03:29.080984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n# Model Selection\n\nPerhaps one of the most important segments of this challenge involves the selection of a model that can successfully handle the given data and yield sensible predictions. Instead of focusing exclusively on your final chosen model, it is also important to share your thought process in this notebook by additionally describing alternative candidate models. There is a wealth of models to choose from, such as decision trees, random forests, (Bayesian) neural networks, Gaussian processes, Lasso regression, and so on. There are several factors which may influence your decision:\n\n-    What is the model's complexity?\n-    Is the model interpretable?\n-    Is the model capable of handling different data-types?\n-    Does the model return uncertainty estimates along with predictions?\n\nIn this baseline solution, we use the Lasso regression model, which is a linear least-square model with L1 regularization on its parameters. There is a hyper-parameter that should be tuned which is the regularization strength α. Intuitively, this hyper-parameter controls the amount of shrinkage of the parameters of the model: the larger the value of α the greater the amount of shrinkage.\n\nSection 3.4.1 of the book The Elements of Statistical Learning: Data Mining, Inference, and Prediction from Trevor Hastie et al. (https://hastie.su.domains/Papers/ESLII.pdf) is a good reference for some classic regression models.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nXtr, Xval, ytr, yval = train_test_split(X, y, random_state=1, test_size=10000)","metadata":{"execution":{"iopub.status.busy":"2023-04-03T13:03:33.090587Z","iopub.execute_input":"2023-04-03T13:03:33.091013Z","iopub.status.idle":"2023-04-03T13:03:33.778416Z","shell.execute_reply.started":"2023-04-03T13:03:33.090976Z","shell.execute_reply":"2023-04-03T13:03:33.776764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import Lasso\n\ndef compute_rmse(y, ypred, ystd=1.):\n    return np.mean((y - ypred)**2)**0.5 * ystd\n\nmodel = Lasso(alpha=1)\nmodel.fit(Xtr, ytr)\nypred_tr = model.predict(Xtr)\nypred_val = model.predict(Xval)\n\nprint(f'Train RMSE: {compute_rmse(ytr, ypred_tr, ystd):.3f}')\nprint(f'Valid RMSE: {compute_rmse(yval, ypred_val, ystd):.3f}')","metadata":{"execution":{"iopub.status.busy":"2023-04-03T13:03:35.008436Z","iopub.execute_input":"2023-04-03T13:03:35.0091Z","iopub.status.idle":"2023-04-03T13:03:35.436598Z","shell.execute_reply.started":"2023-04-03T13:03:35.009061Z","shell.execute_reply":"2023-04-03T13:03:35.43459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Parameter Optimisation\n\nIrrespective of your choice, it is highly likely that your model will have one or more parameters that require tuning. There are several techniques for carrying out such a procedure, including cross-validation, Bayesian optimisation, and several others. As before, an analysis into which parameter tuning technique best suits your model is expected before proceeding with the optimisation of your model.\n\nThe below cells demonstrate tuning the hyper-parameters α of the Ridge regression model by using cross-validation.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV, GridSearchCV","metadata":{"execution":{"iopub.status.busy":"2023-04-03T13:03:43.856743Z","iopub.execute_input":"2023-04-03T13:03:43.857148Z","iopub.status.idle":"2023-04-03T13:03:43.862629Z","shell.execute_reply.started":"2023-04-03T13:03:43.857114Z","shell.execute_reply":"2023-04-03T13:03:43.861381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\ndef get_cv_idx(n, test_size=0.2, n_splits=2):\n    train_idx, test_idx = [], []\n    for _ in range(n_splits):\n        idx = np.random.permutation(n)\n        train_size = int(n * (1 - test_size)) if isinstance(test_size, float) else n - test_size\n        train_idx.append(idx[:train_size])\n        test_idx.append(idx[train_size:])\n    return train_idx, test_idx","metadata":{"execution":{"iopub.status.busy":"2023-04-03T13:03:46.289787Z","iopub.execute_input":"2023-04-03T13:03:46.290205Z","iopub.status.idle":"2023-04-03T13:03:46.297156Z","shell.execute_reply.started":"2023-04-03T13:03:46.290171Z","shell.execute_reply":"2023-04-03T13:03:46.296156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_idx, cv_idx = get_cv_idx(len(Xtr), test_size=10000, n_splits=10)","metadata":{"execution":{"iopub.status.busy":"2023-04-03T13:03:51.422494Z","iopub.execute_input":"2023-04-03T13:03:51.422941Z","iopub.status.idle":"2023-04-03T13:03:52.326506Z","shell.execute_reply.started":"2023-04-03T13:03:51.422886Z","shell.execute_reply":"2023-04-03T13:03:52.3252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_grid = {\n    \"alpha\": [0.001, 0.01, 0.1, 1, 10]\n}\n\nsearch = GridSearchCV(model, \n                      param_grid,\n                      n_jobs=-1, \n                      verbose=1,\n                      cv=zip(train_idx, cv_idx), \n                      scoring='neg_root_mean_squared_error').fit(Xtr, ytr)\nprint('Done!')","metadata":{"execution":{"iopub.status.busy":"2023-04-03T13:03:57.438315Z","iopub.execute_input":"2023-04-03T13:03:57.439692Z","iopub.status.idle":"2023-04-03T13:04:22.667037Z","shell.execute_reply.started":"2023-04-03T13:03:57.439628Z","shell.execute_reply":"2023-04-03T13:04:22.662966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Best parameters set found on cv set:\")\nprint(search.best_params_)\nprint()\nprint(\"Grid scores on cv set:\")\nmeans = search.cv_results_[\"mean_test_score\"]\nstds = search.cv_results_[\"std_test_score\"]\nfor mean, std, params in zip(means, stds, search.cv_results_[\"params\"]):\n    print(\"%0.3f (+/-%0.03f) for %r\" % (-mean * ystd, (std * ystd) * 2, params))\nprint()\nprint(\"Error on the validation set\")\nypred_val = search.predict(Xval)\nprint(f'Valid RMSE: {compute_rmse(yval, ypred_val, ystd):.3f}')","metadata":{"execution":{"iopub.status.busy":"2023-04-03T13:04:22.674335Z","iopub.execute_input":"2023-04-03T13:04:22.675935Z","iopub.status.idle":"2023-04-03T13:04:22.702468Z","shell.execute_reply.started":"2023-04-03T13:04:22.675845Z","shell.execute_reply":"2023-04-03T13:04:22.700988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Evaluation\n\nSome form of pre-evaluation will inevitably be required in the preceding sections in order to both select an appropriate model and configure its parameters appropriately. In this final section, you may evaluate other aspects of the model such as:\n\n-    Assessing the running time of your model;\n-    Determining whether some aspects can be parallelised;\n-    Training the model with smaller subsets of the data.\n-    etc.\n\nRemember, the goal of this challenge is to construct a model for predicting the temperature around the globe.\n\n","metadata":{}},{"cell_type":"code","source":"df_test = pd.read_csv('../input/eurecom-aml-2023-challenge-1/public/test_feat.csv', low_memory=True)","metadata":{"execution":{"iopub.status.busy":"2023-04-03T13:05:13.104613Z","iopub.execute_input":"2023-04-03T13:05:13.105164Z","iopub.status.idle":"2023-04-03T13:05:24.851943Z","shell.execute_reply.started":"2023-04-03T13:05:13.105121Z","shell.execute_reply":"2023-04-03T13:05:24.85064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"col_selection = [\n    'index', 'fact_time', 'fact_latitude', 'fact_longitude',\n    'topography_bathymetry', 'sun_elevation', 'cmc_precipitations',\n    'gfs_a_vorticity', 'gfs_cloudness', 'gfs_clouds_sea', 'gfs_humidity'\n]\n\nXte = df_test[col_selection].iloc[:, 1:].values\nXte = (Xte - Xmean) / Xstd","metadata":{"execution":{"iopub.status.busy":"2023-04-03T13:05:24.854446Z","iopub.execute_input":"2023-04-03T13:05:24.855523Z","iopub.status.idle":"2023-04-03T13:05:24.902197Z","shell.execute_reply.started":"2023-04-03T13:05:24.855471Z","shell.execute_reply":"2023-04-03T13:05:24.900731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ypred_te = search.predict(Xte) * ystd + ymean  # Remember to un-standardize the predictions","metadata":{"execution":{"iopub.status.busy":"2023-04-03T13:05:24.90416Z","iopub.execute_input":"2023-04-03T13:05:24.904852Z","iopub.status.idle":"2023-04-03T13:05:24.922541Z","shell.execute_reply.started":"2023-04-03T13:05:24.904809Z","shell.execute_reply":"2023-04-03T13:05:24.920632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n# Submission\nYour submission is a CSV file containing your final model's predictions on the given test data. This file should contain a header and have the following format:\n\n```\nindex,fact_temperature\n1993574,3.9865149124872303\n1993575,18.165092058370533\n1993576,16.53315442160854\n1993577,8.377598784006866\n...\n```\n\nA leaderboard for this challenge will be ranked using the root mean squared error between the predicted values and the observed arrival delays. However, you can use other metrics for regression tasks in your presentation notebook to evaluate many aspects of your model, including quantification of the uncertainty in the predictions.\n\nBelow is an example of creating a submission file.\n","metadata":{}},{"cell_type":"code","source":"submission_df = pd.DataFrame(data={'index': df_test['index'].values,\n                                   'fact_temperature': ypred_te.squeeze()})\n\n# Save the predictions into a csv file\n# Notice that this file should be saved under the directory `/kaggle/working` \n# so that you can download it later\nsubmission_df.to_csv(\"/kaggle/working/submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-04-03T13:05:24.926708Z","iopub.execute_input":"2023-04-03T13:05:24.927972Z","iopub.status.idle":"2023-04-03T13:05:26.304265Z","shell.execute_reply.started":"2023-04-03T13:05:24.927864Z","shell.execute_reply":"2023-04-03T13:05:26.302846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the submission file\n! head -6 \"/kaggle/working/submission.csv\"","metadata":{"execution":{"iopub.status.busy":"2023-04-03T13:05:26.833994Z","iopub.execute_input":"2023-04-03T13:05:26.834444Z","iopub.status.idle":"2023-04-03T13:05:27.97366Z","shell.execute_reply.started":"2023-04-03T13:05:26.834403Z","shell.execute_reply":"2023-04-03T13:05:27.972082Z"},"trusted":true},"execution_count":null,"outputs":[]}]}