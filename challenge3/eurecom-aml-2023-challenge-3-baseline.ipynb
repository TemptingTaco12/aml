{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1331ab9",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.009031,
     "end_time": "2023-05-22T15:47:54.700245",
     "exception": false,
     "start_time": "2023-05-22T15:47:54.691214",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Example solution for tweet sentiment analysis\n",
    "\n",
    "This is a baseline example to help you with the third challenge. It was originally developed by our Ph.D. student Jonas Wacker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7880be90",
   "metadata": {
    "papermill": {
     "duration": 2.367645,
     "end_time": "2023-05-22T15:47:57.076723",
     "exception": false,
     "start_time": "2023-05-22T15:47:54.709078",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# general NLP preprocessing and basic tools\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "# basic machine learning models\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# our evaluation metric for sentiment classification\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "#from sklearn.pipeline import Pipeline\n",
    "#from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13989271",
   "metadata": {
    "papermill": {
     "duration": 0.0219,
     "end_time": "2023-05-22T15:47:57.107721",
     "exception": false,
     "start_time": "2023-05-22T15:47:57.085821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e81981",
   "metadata": {
    "papermill": {
     "duration": 0.008622,
     "end_time": "2023-05-22T15:47:57.125527",
     "exception": false,
     "start_time": "2023-05-22T15:47:57.116905",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8a4d5ac",
   "metadata": {
    "papermill": {
     "duration": 0.173509,
     "end_time": "2023-05-22T15:47:57.308072",
     "exception": false,
     "start_time": "2023-05-22T15:47:57.134563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('input/eurecom-aml-2023-challenge-3/train.csv')\n",
    "test_df = pd.read_csv('input/eurecom-aml-2023-challenge-3/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df8ad88",
   "metadata": {
    "papermill": {
     "duration": 0.008508,
     "end_time": "2023-05-22T15:47:57.325677",
     "exception": false,
     "start_time": "2023-05-22T15:47:57.317169",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Quick data inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "969e9c1f",
   "metadata": {
    "papermill": {
     "duration": 0.019999,
     "end_time": "2023-05-22T15:47:57.354648",
     "exception": false,
     "start_time": "2023-05-22T15:47:57.334649",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27480"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df)+len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "128fd274",
   "metadata": {
    "papermill": {
     "duration": 0.045008,
     "end_time": "2023-05-22T15:47:57.408807",
     "exception": false,
     "start_time": "2023-05-22T15:47:57.363799",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28ac06f416</td>\n",
       "      <td>good luck with your auction</td>\n",
       "      <td>good luck with your auction</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92098cf9a7</td>\n",
       "      <td>Hmm..You can`t judge a book by looking at its ...</td>\n",
       "      <td>Hmm..You can`t judge a book by looking at its ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7858ff28f2</td>\n",
       "      <td>Hello, yourself. Enjoy London. Watch out for ...</td>\n",
       "      <td>They`re mental.</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b0c9c67f32</td>\n",
       "      <td>We can`t even call you from belgium  sucks</td>\n",
       "      <td>m  suck</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7b36e9e7a5</td>\n",
       "      <td>not so good mood..</td>\n",
       "      <td>not so good mood..</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  28ac06f416                        good luck with your auction   \n",
       "1  92098cf9a7  Hmm..You can`t judge a book by looking at its ...   \n",
       "2  7858ff28f2   Hello, yourself. Enjoy London. Watch out for ...   \n",
       "3  b0c9c67f32         We can`t even call you from belgium  sucks   \n",
       "4  7b36e9e7a5                                 not so good mood..   \n",
       "\n",
       "                                       selected_text sentiment  \n",
       "0                        good luck with your auction  positive  \n",
       "1  Hmm..You can`t judge a book by looking at its ...   neutral  \n",
       "2                                    They`re mental.  negative  \n",
       "3                                            m  suck  negative  \n",
       "4                                 not so good mood..  negative  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2990d54d",
   "metadata": {
    "papermill": {
     "duration": 0.02434,
     "end_time": "2023-05-22T15:47:57.442741",
     "exception": false,
     "start_time": "2023-05-22T15:47:57.418401",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>102f98e5e2</td>\n",
       "      <td>Happy Mother`s Day hahaha</td>\n",
       "      <td>Happy Mother`s Day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>033b399113</td>\n",
       "      <td>Sorry for the triple twitter post, was having ...</td>\n",
       "      <td>Sorry for the triple twitter post, was having ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c125e29be2</td>\n",
       "      <td>thats much better than the flu syndrome!</td>\n",
       "      <td>thats much better</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b91e2b0679</td>\n",
       "      <td>Aww I have a tummy ache</td>\n",
       "      <td>tummy ache</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1a46141274</td>\n",
       "      <td>hey chocolate chips is good.  i want a snack ...</td>\n",
       "      <td>good.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  102f98e5e2                          Happy Mother`s Day hahaha   \n",
       "1  033b399113  Sorry for the triple twitter post, was having ...   \n",
       "2  c125e29be2           thats much better than the flu syndrome!   \n",
       "3  b91e2b0679                            Aww I have a tummy ache   \n",
       "4  1a46141274   hey chocolate chips is good.  i want a snack ...   \n",
       "\n",
       "                                       selected_text  \n",
       "0                                 Happy Mother`s Day  \n",
       "1  Sorry for the triple twitter post, was having ...  \n",
       "2                                  thats much better  \n",
       "3                                         tummy ache  \n",
       "4                                              good.  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac212b87",
   "metadata": {
    "papermill": {
     "duration": 0.009056,
     "end_time": "2023-05-22T15:47:57.461623",
     "exception": false,
     "start_time": "2023-05-22T15:47:57.452567",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dc776a9",
   "metadata": {
    "papermill": {
     "duration": 0.036337,
     "end_time": "2023-05-22T15:47:57.507649",
     "exception": false,
     "start_time": "2023-05-22T15:47:57.471312",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we create a validation dataset from the training data\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbea491",
   "metadata": {
    "papermill": {
     "duration": 0.00891,
     "end_time": "2023-05-22T15:47:57.525942",
     "exception": false,
     "start_time": "2023-05-22T15:47:57.517032",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We start off by converting the labels to numbers. This is a requirement for the submission and numerical inputs are generally more compatible with machine learning libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90dddd24",
   "metadata": {
    "papermill": {
     "duration": 0.019636,
     "end_time": "2023-05-22T15:47:57.554763",
     "exception": false,
     "start_time": "2023-05-22T15:47:57.535127",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_conversion = {\n",
    "    'neutral': 0,\n",
    "    'positive': 1,\n",
    "    'negative': -1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67e90ada",
   "metadata": {
    "papermill": {
     "duration": 0.028105,
     "end_time": "2023-05-22T15:47:57.592571",
     "exception": false,
     "start_time": "2023-05-22T15:47:57.564466",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df['target'] = train_df['sentiment'].map(target_conversion)\n",
    "val_df['target'] = val_df['sentiment'].map(target_conversion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d2a5a4",
   "metadata": {
    "papermill": {
     "duration": 0.00955,
     "end_time": "2023-05-22T15:47:57.611893",
     "exception": false,
     "start_time": "2023-05-22T15:47:57.602343",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now we need to find a numerical representation for our input data. Extracting features from text is one of the major building blocks of any Natural Language Processing (NLP) pipeline.\n",
    "\n",
    "There have been huge developments in the field during the last decade. A very traditional approach is to extract Bag-of-Words features. See here for an explanation:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\n",
    "\n",
    "We will stick to this technique for the purpose of this example notebook. However, be aware that much more powerful feature extraction techniques exist. The most recent ones use neural network based language models. See e.g.:\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2019/06/understanding-transformers-nlp-state-of-the-art-models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86987571",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\laura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\laura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')  # Download the tokenizer resource\n",
    "nltk.download('stopwords') # Download the stopwords resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce9a0dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tokenizer function\n",
    "def custom_tokenizer(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    #tokens = [token for token in tokens if token.lower() not in stop_words] # Apply lowercasing to each token not in stop words\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecaf9630",
   "metadata": {
    "papermill": {
     "duration": 0.019726,
     "end_time": "2023-05-22T15:47:57.641609",
     "exception": false,
     "start_time": "2023-05-22T15:47:57.621883",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specifiies the n-gram range we want to extract\n",
    "ngram_range = (1, 3)\n",
    "\n",
    "count_vect = CountVectorizer(\n",
    "    tokenizer=custom_tokenizer \n",
    "    #stop_words=set(stopwords.words('english'))\n",
    "    #ngram_range=ngram_range\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6407f98f-3b57-47f7-a58e-a3ec7c5d80b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b6fae5c",
   "metadata": {
    "papermill": {
     "duration": 0.487059,
     "end_time": "2023-05-22T15:47:58.139407",
     "exception": false,
     "start_time": "2023-05-22T15:47:57.652348",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# here we are obtaining the vocabulary from the training data minus validation data\n",
    "# you may want to change this to the full training data for the final submission\n",
    "X_train_counts = count_vect.fit_transform(list(train_df['text'].values))\n",
    "X_val_counts = count_vect.transform(list(val_df['text'].values))\n",
    "X_test_counts = count_vect.transform(list(test_df['text'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "601bef13",
   "metadata": {
    "papermill": {
     "duration": 0.019339,
     "end_time": "2023-05-22T15:47:58.167921",
     "exception": false,
     "start_time": "2023-05-22T15:47:58.148582",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train feature shape: (22258, 23239)\n",
      "Validation feature shape: (2474, 23239)\n",
      "Test feature shape: (2748, 23239)\n"
     ]
    }
   ],
   "source": [
    "print('Train feature shape:', X_train_counts.shape)\n",
    "print('Validation feature shape:', X_val_counts.shape)\n",
    "print('Test feature shape:', X_test_counts.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa6fde5",
   "metadata": {
    "papermill": {
     "duration": 0.009659,
     "end_time": "2023-05-22T15:47:58.187642",
     "exception": false,
     "start_time": "2023-05-22T15:47:58.177983",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The Bag-of-Words representation assigns a unique ID to each word that appears in the training data. 23239 unique words have been extracted. Each input data point (tweet) is then represented by a vector of the size of the vocabulary. Each of its elements are the counts of the respective word appearing in the tweet.\n",
    "\n",
    "Therefore, the features have a huge dimension! Storing the feature matrix directly would require (n_datapoints x vocabulary size) * 32 bits  â‰ˆ\n",
    "  2 GB CPU/GPU RAM! Imagine we were not analyzing tweets (limited vocabulary) but Wikipedia! Or imagine we had a larger corpus of documents. Then we could not store the features!\n",
    "\n",
    "Instead, the Bag-of-Words features are usually stored using a sparse representation. Imagine this like a dictionary of ID-count tuples assigned to each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e790898",
   "metadata": {
    "papermill": {
     "duration": 0.020285,
     "end_time": "2023-05-22T15:47:58.217887",
     "exception": false,
     "start_time": "2023-05-22T15:47:58.197602",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<22258x23239 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 255005 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we quickly analyze the matrix of word counts:\n",
    "# Only 255125 of the 22258x23162 values => 0.049487% are non-zero.\n",
    "# The sparse encoding only needs to store these.\n",
    "X_train_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1c86674",
   "metadata": {
    "papermill": {
     "duration": 0.024178,
     "end_time": "2023-05-22T15:47:58.251391",
     "exception": false,
     "start_time": "2023-05-22T15:47:58.227213",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Yet, we can ask to convert a part of the matrix into the traditional dense format.\n",
    "# It's quite challenging to find any non-zeros here!\n",
    "X_train_counts[:10,:10].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bcfed126",
   "metadata": {
    "papermill": {
     "duration": 0.022665,
     "end_time": "2023-05-22T15:47:58.284408",
     "exception": false,
     "start_time": "2023-05-22T15:47:58.261743",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18618"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The other way around is easier. We can ask to find the ID (index) of a specific word.\n",
    "count_vect.vocabulary_.get('sleep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35054083",
   "metadata": {
    "papermill": {
     "duration": 0.020809,
     "end_time": "2023-05-22T15:47:58.314800",
     "exception": false,
     "start_time": "2023-05-22T15:47:58.293991",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet:\n",
      " had a horrible sleep + in a rather bad mood\n",
      "Number of times the word \"sleep\" appeared:\n",
      " 1\n"
     ]
    }
   ],
   "source": [
    "# So the first tweet should have a one at this position:\n",
    "print('Tweet:\\n', train_df.iloc[0]['text'])\n",
    "print('Number of times the word \"sleep\" appeared:\\n', X_train_counts[0, 18618])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189ac5bb",
   "metadata": {
    "papermill": {
     "duration": 0.009336,
     "end_time": "2023-05-22T15:47:58.334232",
     "exception": false,
     "start_time": "2023-05-22T15:47:58.324896",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training a simple classifier\n",
    "\n",
    "We are training a naive Bayes classifier on the Bag-of-Words features of the training data:\n",
    "\n",
    "https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html\n",
    "\n",
    "It is already built into the sklearn library:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\n",
    "\n",
    "Keep in mind that not only storing the features is challenging but also processing them. A simple SVM may be quite slow on such high-dimensional features. Naive Bayes works well with Bag-of-Words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b0eaf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation of hyperparameters\n",
    "\n",
    "# pipeline = Pipeline([\n",
    "#     ('vectorizer', CountVectorizer(tokenizer=custom_tokenizer)),\n",
    "#     ('classifier', MultinomialNB())\n",
    "# ])\n",
    "\n",
    "# # Defines the parameter grid to search over\n",
    "# param_grid = {\n",
    "#     'vectorizer__ngram_range': [(1, 1), \n",
    "#                                 (1, 2), \n",
    "#                                 (1, 3),\n",
    "#                                 (2, 2),\n",
    "#                                 (2, 3),\n",
    "#                                 (3, 3)]  # Example ngram_range values\n",
    "# }\n",
    "\n",
    "# grid_search = GridSearchCV(pipeline, param_grid, scoring='f1_macro', cv=10)\n",
    "\n",
    "# grid_search.fit(train_df['text'], train_df['target'])\n",
    "\n",
    "# print(\"Best ngram_range:\", grid_search.best_params_['vectorizer__ngram_range'])\n",
    "# print(\"Best F1 score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb57d226-64ea-446b-93e3-c459aefc562b",
   "metadata": {},
   "source": [
    "## Trying stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ae6134f-23d2-4ecf-bca4-cbfb5c09f730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\laura\\anaconda3\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\laura\\anaconda3\\lib\\site-packages (from torch) (1.10.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\laura\\anaconda3\\lib\\site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\laura\\anaconda3\\lib\\site-packages (from torch) (4.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\laura\\anaconda3\\lib\\site-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torchtext in c:\\users\\laura\\anaconda3\\lib\\site-packages (0.15.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\laura\\anaconda3\\lib\\site-packages (from torchtext) (4.64.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\laura\\anaconda3\\lib\\site-packages (from torchtext) (1.23.5)\n",
      "Requirement already satisfied: requests in c:\\users\\laura\\anaconda3\\lib\\site-packages (from torchtext) (2.28.1)\n",
      "Requirement already satisfied: torchdata==0.6.1 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from torchtext) (0.6.1)\n",
      "Requirement already satisfied: torch==2.0.1 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from torchtext) (2.0.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\laura\\anaconda3\\lib\\site-packages (from torch==2.0.1->torchtext) (3.6.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from torch==2.0.1->torchtext) (2.11.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\laura\\anaconda3\\lib\\site-packages (from torch==2.0.1->torchtext) (1.10.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\laura\\anaconda3\\lib\\site-packages (from torch==2.0.1->torchtext) (4.3.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\laura\\anaconda3\\lib\\site-packages (from torch==2.0.1->torchtext) (2.8.4)\n",
      "Requirement already satisfied: urllib3>=1.25 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from torchdata==0.6.1->torchtext) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from requests->torchtext) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from requests->torchtext) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from requests->torchtext) (2022.9.14)\n",
      "Requirement already satisfied: colorama in c:\\users\\laura\\anaconda3\\lib\\site-packages (from tqdm->torchtext) (0.4.5)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from jinja2->torch==2.0.1->torchtext) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from sympy->torch==2.0.1->torchtext) (1.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers in c:\\users\\laura\\anaconda3\\lib\\site-packages (4.29.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: filelock in c:\\users\\laura\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: requests in c:\\users\\laura\\anaconda3\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.3.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\laura\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2022.7.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\laura\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.9.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: datasets in c:\\users\\laura\\anaconda3\\lib\\site-packages (2.12.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\laura\\anaconda3\\lib\\site-packages (from datasets) (1.4.4)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from datasets) (2022.7.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\laura\\anaconda3\\lib\\site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\laura\\anaconda3\\lib\\site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from datasets) (1.23.5)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\laura\\anaconda3\\lib\\site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\laura\\anaconda3\\lib\\site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from datasets) (0.15.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from datasets) (12.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\laura\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2022.9.14)\n",
      "Requirement already satisfied: colorama in c:\\users\\laura\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\laura\\appdata\\roaming\\python\\python39\\site-packages (2.12.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.12.0 in c:\\users\\laura\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (2.12.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\laura\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (21.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\laura\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (16.0.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\laura\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.54.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (4.22.3)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\laura\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: jax>=0.3.15 in c:\\users\\laura\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: numpy<1.24,>=1.22 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.23.5)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (23.3.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (4.3.0)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in c:\\users\\laura\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\laura\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\laura\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\laura\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (59.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\laura\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.4.0)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in c:\\users\\laura\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.12.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: scipy>=1.7 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow) (1.9.1)\n",
      "Requirement already satisfied: ml-dtypes>=0.0.3 in c:\\users\\laura\\appdata\\roaming\\python\\python39\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.7.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\laura\\appdata\\roaming\\python\\python39\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.17.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\laura\\appdata\\roaming\\python\\python39\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from packaging->tensorflow-intel==2.12.0->tensorflow) (3.0.9)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\laura\\appdata\\roaming\\python\\python39\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\laura\\appdata\\roaming\\python\\python39\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\laura\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch\n",
    "%pip install torchtext\n",
    "%pip install transformers\n",
    "%pip install datasets\n",
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb76008a-862c-4495-b826-078c0849fb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "from transformers import AutoConfig, AutoModel\n",
    "from transformers import DataCollatorWithPadding\n",
    "from datasets import load_metric\n",
    "from evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bbf428a0-99b8-4c36-97c3-c9f647e83ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n",
      "Xformers is not installed correctly. If you want to use memorry_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "pretrained_model = pipeline(\"sentiment-analysis\", model=\"finiteautomata/bertweet-base-sentiment-analysis\")\n",
    "output = pretrained_model(list(train_df['text'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e82e69aa-fd1d-4aea-9f8f-8f297f28faec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Prediction Score: 0.8591309786378475\n"
     ]
    }
   ],
   "source": [
    "total_score = sum(result['score'] for result in output)\n",
    "average_score = total_score / len(output)\n",
    "print(f\"Average Prediction Score: {average_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9a8c8bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = pd.DataFrame(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "60d3c1d8-1576-4af4-a03d-c5a01ac63be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "   load_accuracy = load_metric(\"accuracy\")\n",
    "   load_f1 = load_metric(\"f1\")\n",
    "  \n",
    "   logits, labels = eval_pred\n",
    "   predictions = np.argmax(logits, axis=-1)\n",
    "   accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "   f1 = load_f1.compute(predictions=predictions, references=labels)[\"f1\"]\n",
    "   return {\"accuracy\": accuracy, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac71c27-56bf-4b88-a811-7802ea7c4106",
   "metadata": {},
   "source": [
    "## End of trying stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3c58f4-0c1d-4944-ad1d-6d2f0eea4c76",
   "metadata": {
    "papermill": {
     "duration": 0.03574,
     "end_time": "2023-05-22T15:47:58.379558",
     "exception": false,
     "start_time": "2023-05-22T15:47:58.343818",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "clf = MultinomialNB().fit(X_train_counts, train_df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feaac84",
   "metadata": {
    "papermill": {
     "duration": 0.020647,
     "end_time": "2023-05-22T15:47:58.410499",
     "exception": false,
     "start_time": "2023-05-22T15:47:58.389852",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_predictions_nb = clf.predict(X_val_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e02a8ef",
   "metadata": {
    "papermill": {
     "duration": 0.020954,
     "end_time": "2023-05-22T15:47:58.442135",
     "exception": false,
     "start_time": "2023-05-22T15:47:58.421181",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracy = (val_predictions_nb == val_df['target'].values).mean()\n",
    "print('The accuracy of our multinomial Naive Bayes classifier is: {:.2f}%'.format(accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16937191",
   "metadata": {
    "papermill": {
     "duration": 0.025117,
     "end_time": "2023-05-22T15:47:58.477728",
     "exception": false,
     "start_time": "2023-05-22T15:47:58.452611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fbeta = fbeta_score(val_df['target'].values, val_predictions_nb, average='macro', beta=1.0)\n",
    "print('The fbeta score is:', fbeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a493e0",
   "metadata": {
    "papermill": {
     "duration": 0.496208,
     "end_time": "2023-05-22T15:47:58.984220",
     "exception": false,
     "start_time": "2023-05-22T15:47:58.488012",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating a submission\n",
    "\n",
    "X_train_counts = count_vect.fit_transform(list(train_df['text'].values) + list(val_df['text'].values))\n",
    "X_test_counts = count_vect.transform(list(test_df['text'].values))\n",
    "\n",
    "clf = MultinomialNB().fit(X_train_counts, np.hstack([train_df['target'].values, val_df['target'].values]))\n",
    "test_predictions_nb = clf.predict(X_test_counts)\n",
    "\n",
    "submission_df = pd.DataFrame()\n",
    "submission_df['textID'] = test_df['textID']\n",
    "submission_df['sentiment'] = test_predictions_nb\n",
    "submission_df.to_csv('TA_baseline_NB.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df01cbfe",
   "metadata": {
    "papermill": {
     "duration": 0.011562,
     "end_time": "2023-05-22T15:47:59.005787",
     "exception": false,
     "start_time": "2023-05-22T15:47:58.994225",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## How good is this score?\n",
    "\n",
    "Early approaches in NLP used rule-based classifiers for sentiment analysis. A popular baseline is VADER which was published in 2014:\n",
    "\n",
    "https://www.aaai.org/ocs/index.php/ICWSM/ICWSM14/paper/viewPaper/8109\n",
    "\n",
    "VADER does not use any machine learning but is purely handcrafted by humans. It uses text preprocessing and lexica to determine the sentiment of a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c31cc37",
   "metadata": {
    "papermill": {
     "duration": 0.202593,
     "end_time": "2023-05-22T15:47:59.218487",
     "exception": false,
     "start_time": "2023-05-22T15:47:59.015894",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f0b93d",
   "metadata": {
    "papermill": {
     "duration": 0.035011,
     "end_time": "2023-05-22T15:47:59.263978",
     "exception": false,
     "start_time": "2023-05-22T15:47:59.228967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d77c810",
   "metadata": {
    "papermill": {
     "duration": 0.022,
     "end_time": "2023-05-22T15:47:59.297174",
     "exception": false,
     "start_time": "2023-05-22T15:47:59.275174",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We show a few prediction examples:\n",
    "for doc in val_df['text'].iloc[:5].values:\n",
    "    print(doc)\n",
    "    print(sid.polarity_scores(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478bc6b8",
   "metadata": {
    "papermill": {
     "duration": 0.02075,
     "end_time": "2023-05-22T15:47:59.329099",
     "exception": false,
     "start_time": "2023-05-22T15:47:59.308349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def vader_predict(x):\n",
    "    prediction = sid.polarity_scores(x)\n",
    "    prediction_list = [\n",
    "        (1, prediction['pos']),\n",
    "        (-1, prediction['neg']),\n",
    "        (0, prediction['neu'])\n",
    "    ]\n",
    "    label = sorted(prediction_list, key=lambda x: x[1], reverse=True)[0][0]\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a58f284",
   "metadata": {
    "papermill": {
     "duration": 0.536793,
     "end_time": "2023-05-22T15:47:59.876745",
     "exception": false,
     "start_time": "2023-05-22T15:47:59.339952",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions_vader = val_df['text'].apply(vader_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d3c297",
   "metadata": {
    "papermill": {
     "duration": 0.026732,
     "end_time": "2023-05-22T15:47:59.919575",
     "exception": false,
     "start_time": "2023-05-22T15:47:59.892843",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracy = (predictions_vader == val_df['target'].values).mean()\n",
    "print('The accuracy of VADER is: {:.2f}%'.format(accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940e25c4",
   "metadata": {
    "papermill": {
     "duration": 0.037133,
     "end_time": "2023-05-22T15:47:59.972045",
     "exception": false,
     "start_time": "2023-05-22T15:47:59.934912",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fbeta = fbeta_score(val_df['target'].values, predictions_vader, average='macro', beta=1.0)\n",
    "print('The fbeta score is:', fbeta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1a25d7",
   "metadata": {
    "papermill": {
     "duration": 0.012881,
     "end_time": "2023-05-22T15:47:59.996514",
     "exception": false,
     "start_time": "2023-05-22T15:47:59.983633",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "VADER performs worse! That is a good sign that our classifier learned useful generalizations from the training data (better than standard handcrafted rules)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e64845",
   "metadata": {
    "papermill": {
     "duration": 0.011512,
     "end_time": "2023-05-22T15:48:00.019748",
     "exception": false,
     "start_time": "2023-05-22T15:48:00.008236",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Where to go from here?\n",
    "\n",
    "We can improve our Machine Learning pipeline on multiple aspects:\n",
    "\n",
    "### Data analysis:\n",
    "How is the data distributed? Can we analyze our data to find patterns associated with the classes? Which kinds of words are useful, which aren't?\n",
    "\n",
    "### Feature extraction:\n",
    "Can we make our Bag-of-Words representation more compact or richer? There are many things you could try to implement. Here are some buzzwords: tokenization, stop words removal, lemmatization, n-gram extraction, ...\n",
    "A useful Python library to address these issues is: NLTK (https://www.nltk.org/)\n",
    "The sklearn CountVectorizer we used can be combined with NLTK preprocessing: https://scikit-learn.org/stable/modules/feature_extraction.html#customizing-the-vectorizer-classes\n",
    "Is there also a dense (as opposed to sparse) representation of documents (tweets in our case)? Buzzwords: word2vec, gloVe\n",
    "The state-of-the-art: ... are neural network language models, so-called Transformers. There are pretrained models available. If you feel comfortable with neural networks, fine-tuning and GPUs, have a look here: https://huggingface.co/transformers/\n",
    "\n",
    "In general, we also recommend spaCy as a convenient Python library that covers most of the above features at once and may be a great resource to start with: https://spacy.io/\n",
    "\n",
    "### Model selection:\n",
    "The model of choice highly depends on the previously extracted features. Depending on whether you obtain a sparse or dense feature representation, you have to choose an appropriate model!\n",
    "\n",
    "### Model evaluation:\n",
    "Make sure to select potential model hyperparameters using cross-validation or similar. Our evaluation metric of choice is the F1-score:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html#sklearn.metrics.fbeta_score\n",
    "\n",
    "We choose beta=1 and average=macro\n",
    "\n",
    "### Extension idea 1:\n",
    "Apart from classifying the sentiment of tweets, we can also try to determine which words are the reason for the classifier to determine the classification. Ground-truth labels for these words are contained in our training data. The evaluation will not take place on the Kaggle platform. You need to do it yourself. Use the Jaccard coefficient to evaluate the overlap between the selected words and the ground truth:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html#jaccard-similarity-coefficient-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372d0824",
   "metadata": {
    "papermill": {
     "duration": 0.030136,
     "end_time": "2023-05-22T15:48:00.064047",
     "exception": false,
     "start_time": "2023-05-22T15:48:00.033911",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# selected_text shows the words selected from text to lead to the classification stored in sentiment\n",
    "train_df[['text', 'selected_text', 'sentiment']].iloc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a2aa08",
   "metadata": {
    "papermill": {
     "duration": 0.01657,
     "end_time": "2023-05-22T15:48:00.097195",
     "exception": false,
     "start_time": "2023-05-22T15:48:00.080625",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Extension idea 2:\n",
    "\n",
    "You may want to give it a try to Kaggle's brand new feature called models!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 17.636291,
   "end_time": "2023-05-22T15:48:01.235515",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-05-22T15:47:43.599224",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
