{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b1331ab9",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.009031,
     "end_time": "2023-05-22T15:47:54.700245",
     "exception": false,
     "start_time": "2023-05-22T15:47:54.691214",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Example solution for tweet sentiment analysis\n",
    "\n",
    "This is a baseline example to help you with the third challenge. It was originally developed by our Ph.D. student Jonas Wacker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "7880be90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-22T15:47:54.721023Z",
     "iopub.status.busy": "2023-05-22T15:47:54.719768Z",
     "iopub.status.idle": "2023-05-22T15:47:57.073746Z",
     "shell.execute_reply": "2023-05-22T15:47:57.072336Z"
    },
    "papermill": {
     "duration": 2.367645,
     "end_time": "2023-05-22T15:47:57.076723",
     "exception": false,
     "start_time": "2023-05-22T15:47:54.709078",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# general NLP preprocessing and basic tools\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "# basic machine learning models\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# our evaluation metric for sentiment classification\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "13989271",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-22T15:47:57.097560Z",
     "iopub.status.busy": "2023-05-22T15:47:57.097072Z",
     "iopub.status.idle": "2023-05-22T15:47:57.105300Z",
     "shell.execute_reply": "2023-05-22T15:47:57.103815Z"
    },
    "papermill": {
     "duration": 0.0219,
     "end_time": "2023-05-22T15:47:57.107721",
     "exception": false,
     "start_time": "2023-05-22T15:47:57.085821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23e81981",
   "metadata": {
    "papermill": {
     "duration": 0.008622,
     "end_time": "2023-05-22T15:47:57.125527",
     "exception": false,
     "start_time": "2023-05-22T15:47:57.116905",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f8a4d5ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-22T15:47:57.145295Z",
     "iopub.status.busy": "2023-05-22T15:47:57.144882Z",
     "iopub.status.idle": "2023-05-22T15:47:57.305174Z",
     "shell.execute_reply": "2023-05-22T15:47:57.304043Z"
    },
    "papermill": {
     "duration": 0.173509,
     "end_time": "2023-05-22T15:47:57.308072",
     "exception": false,
     "start_time": "2023-05-22T15:47:57.134563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('kaggle/input/eurecom-aml-2023-challenge-3/train.csv')\n",
    "test_df = pd.read_csv('kaggle/input/eurecom-aml-2023-challenge-3/test.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8df8ad88",
   "metadata": {
    "papermill": {
     "duration": 0.008508,
     "end_time": "2023-05-22T15:47:57.325677",
     "exception": false,
     "start_time": "2023-05-22T15:47:57.317169",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Quick data inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "969e9c1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-22T15:47:57.345341Z",
     "iopub.status.busy": "2023-05-22T15:47:57.344921Z",
     "iopub.status.idle": "2023-05-22T15:47:57.352338Z",
     "shell.execute_reply": "2023-05-22T15:47:57.351552Z"
    },
    "papermill": {
     "duration": 0.019999,
     "end_time": "2023-05-22T15:47:57.354648",
     "exception": false,
     "start_time": "2023-05-22T15:47:57.334649",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27480"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df)+len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "128fd274",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-22T15:47:57.375287Z",
     "iopub.status.busy": "2023-05-22T15:47:57.374488Z",
     "iopub.status.idle": "2023-05-22T15:47:57.406437Z",
     "shell.execute_reply": "2023-05-22T15:47:57.405493Z"
    },
    "papermill": {
     "duration": 0.045008,
     "end_time": "2023-05-22T15:47:57.408807",
     "exception": false,
     "start_time": "2023-05-22T15:47:57.363799",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28ac06f416</td>\n",
       "      <td>good luck with your auction</td>\n",
       "      <td>good luck with your auction</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92098cf9a7</td>\n",
       "      <td>Hmm..You can`t judge a book by looking at its ...</td>\n",
       "      <td>Hmm..You can`t judge a book by looking at its ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7858ff28f2</td>\n",
       "      <td>Hello, yourself. Enjoy London. Watch out for ...</td>\n",
       "      <td>They`re mental.</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b0c9c67f32</td>\n",
       "      <td>We can`t even call you from belgium  sucks</td>\n",
       "      <td>m  suck</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7b36e9e7a5</td>\n",
       "      <td>not so good mood..</td>\n",
       "      <td>not so good mood..</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  28ac06f416                        good luck with your auction   \n",
       "1  92098cf9a7  Hmm..You can`t judge a book by looking at its ...   \n",
       "2  7858ff28f2   Hello, yourself. Enjoy London. Watch out for ...   \n",
       "3  b0c9c67f32         We can`t even call you from belgium  sucks   \n",
       "4  7b36e9e7a5                                 not so good mood..   \n",
       "\n",
       "                                       selected_text sentiment  \n",
       "0                        good luck with your auction  positive  \n",
       "1  Hmm..You can`t judge a book by looking at its ...   neutral  \n",
       "2                                    They`re mental.  negative  \n",
       "3                                            m  suck  negative  \n",
       "4                                 not so good mood..  negative  "
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "2990d54d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-22T15:47:57.429632Z",
     "iopub.status.busy": "2023-05-22T15:47:57.429247Z",
     "iopub.status.idle": "2023-05-22T15:47:57.439949Z",
     "shell.execute_reply": "2023-05-22T15:47:57.438864Z"
    },
    "papermill": {
     "duration": 0.02434,
     "end_time": "2023-05-22T15:47:57.442741",
     "exception": false,
     "start_time": "2023-05-22T15:47:57.418401",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>102f98e5e2</td>\n",
       "      <td>Happy Mother`s Day hahaha</td>\n",
       "      <td>Happy Mother`s Day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>033b399113</td>\n",
       "      <td>Sorry for the triple twitter post, was having ...</td>\n",
       "      <td>Sorry for the triple twitter post, was having ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c125e29be2</td>\n",
       "      <td>thats much better than the flu syndrome!</td>\n",
       "      <td>thats much better</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b91e2b0679</td>\n",
       "      <td>Aww I have a tummy ache</td>\n",
       "      <td>tummy ache</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1a46141274</td>\n",
       "      <td>hey chocolate chips is good.  i want a snack ...</td>\n",
       "      <td>good.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  102f98e5e2                          Happy Mother`s Day hahaha   \n",
       "1  033b399113  Sorry for the triple twitter post, was having ...   \n",
       "2  c125e29be2           thats much better than the flu syndrome!   \n",
       "3  b91e2b0679                            Aww I have a tummy ache   \n",
       "4  1a46141274   hey chocolate chips is good.  i want a snack ...   \n",
       "\n",
       "                                       selected_text  \n",
       "0                                 Happy Mother`s Day  \n",
       "1  Sorry for the triple twitter post, was having ...  \n",
       "2                                  thats much better  \n",
       "3                                         tummy ache  \n",
       "4                                              good.  "
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ac212b87",
   "metadata": {
    "papermill": {
     "duration": 0.009056,
     "end_time": "2023-05-22T15:47:57.461623",
     "exception": false,
     "start_time": "2023-05-22T15:47:57.452567",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "5dc776a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-22T15:47:57.483821Z",
     "iopub.status.busy": "2023-05-22T15:47:57.482781Z",
     "iopub.status.idle": "2023-05-22T15:47:57.504979Z",
     "shell.execute_reply": "2023-05-22T15:47:57.503784Z"
    },
    "papermill": {
     "duration": 0.036337,
     "end_time": "2023-05-22T15:47:57.507649",
     "exception": false,
     "start_time": "2023-05-22T15:47:57.471312",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we create a validation dataset from the training data\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1bbea491",
   "metadata": {
    "papermill": {
     "duration": 0.00891,
     "end_time": "2023-05-22T15:47:57.525942",
     "exception": false,
     "start_time": "2023-05-22T15:47:57.517032",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We start off by converting the labels to numbers. This is a requirement for the submission and numerical inputs are generally more compatible with machine learning libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "90dddd24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-22T15:47:57.546295Z",
     "iopub.status.busy": "2023-05-22T15:47:57.545882Z",
     "iopub.status.idle": "2023-05-22T15:47:57.552136Z",
     "shell.execute_reply": "2023-05-22T15:47:57.550347Z"
    },
    "papermill": {
     "duration": 0.019636,
     "end_time": "2023-05-22T15:47:57.554763",
     "exception": false,
     "start_time": "2023-05-22T15:47:57.535127",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_conversion = {\n",
    "    'neutral': 0,\n",
    "    'positive': 1,\n",
    "    'negative': -1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "67e90ada",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-22T15:47:57.576102Z",
     "iopub.status.busy": "2023-05-22T15:47:57.575668Z",
     "iopub.status.idle": "2023-05-22T15:47:57.589589Z",
     "shell.execute_reply": "2023-05-22T15:47:57.588341Z"
    },
    "papermill": {
     "duration": 0.028105,
     "end_time": "2023-05-22T15:47:57.592571",
     "exception": false,
     "start_time": "2023-05-22T15:47:57.564466",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df['target'] = train_df['sentiment'].map(target_conversion)\n",
    "val_df['target'] = val_df['sentiment'].map(target_conversion)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "99d2a5a4",
   "metadata": {
    "papermill": {
     "duration": 0.00955,
     "end_time": "2023-05-22T15:47:57.611893",
     "exception": false,
     "start_time": "2023-05-22T15:47:57.602343",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now we need to find a numerical representation for our input data. Extracting features from text is one of the major building blocks of any Natural Language Processing (NLP) pipeline.\n",
    "\n",
    "There have been huge developments in the field during the last decade. A very traditional approach is to extract Bag-of-Words features. See here for an explanation:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\n",
    "\n",
    "We will stick to this technique for the purpose of this example notebook. However, be aware that much more powerful feature extraction techniques exist. The most recent ones use neural network based language models. See e.g.:\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2019/06/understanding-transformers-nlp-state-of-the-art-models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "86987571",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ab33068/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ab33068/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')  # Download the tokenizer resource\n",
    "nltk.download('stopwords') # Download the stopwords resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "ce9a0dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom tokenizer function\n",
    "def custom_tokenizer(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    #tokens = [token for token in tokens if token.lower() not in stop_words] # Apply lowercasing to each token not in stop words\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "# Define pretrained tokenizer function\n",
    "def auto_tokenizer(text):\n",
    "    bert_uncased_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', add_special_tokens=True, use_fast=True)\n",
    "    # bert_cased_tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "    # roberta_base_tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "    # roberta_large_tokenizer = AutoTokenizer.from_pretrained('roberta-large')\n",
    "    # gpt_tokenizer = AutoTokenizer.from_pretrained('gpt2', add_special_tokens=True, use_fast=True)\n",
    "\n",
    "    tokens = bert_uncased_tokenizer.tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "9b0eaf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation of hyperparameters\n",
    "\n",
    "# pipeline = Pipeline([\n",
    "#     ('vectorizer', CountVectorizer(tokenizer=custom_tokenizer)),\n",
    "#     ('classifier', MultinomialNB())\n",
    "# ])\n",
    "\n",
    "# Defines the parameter grid to search over\n",
    "# param_grid = {\n",
    "#     'vectorizer__ngram_range': [(1, 1), \n",
    "#                                 (1, 2), \n",
    "#                                 (1, 3),\n",
    "#                                 (1, 4),\n",
    "#                                 (2, 2),\n",
    "#                                 (2, 3),\n",
    "#                                 (2, 4),\n",
    "#                                 (3, 3),\n",
    "#                                 (3, 4),\n",
    "#                                 (4, 4)]\n",
    "# }\n",
    "\n",
    "# grid_search = GridSearchCV(pipeline, param_grid, scoring='f1_macro', cv=10)\n",
    "\n",
    "# grid_search.fit(train_df['text'], train_df['target'])\n",
    "\n",
    "# print(\"Best ngram_range:\", grid_search.best_params_['vectorizer__ngram_range'])\n",
    "# print(\"Best F1 score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ecaf9630",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-22T15:47:57.633070Z",
     "iopub.status.busy": "2023-05-22T15:47:57.632620Z",
     "iopub.status.idle": "2023-05-22T15:47:57.638947Z",
     "shell.execute_reply": "2023-05-22T15:47:57.637665Z"
    },
    "papermill": {
     "duration": 0.019726,
     "end_time": "2023-05-22T15:47:57.641609",
     "exception": false,
     "start_time": "2023-05-22T15:47:57.621883",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specifiies the n-gram range we want to extract\n",
    "ngram_range = (1, 3)\n",
    "\n",
    "count_vect = CountVectorizer(\n",
    "    tokenizer=auto_tokenizer, \n",
    "    #tokenizer=custom_tokenizer, \n",
    "    #stop_words=set(stopwords.words('english'))\n",
    "    #ngram_range=ngram_range\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "6b6fae5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-22T15:47:57.665172Z",
     "iopub.status.busy": "2023-05-22T15:47:57.663976Z",
     "iopub.status.idle": "2023-05-22T15:47:58.136571Z",
     "shell.execute_reply": "2023-05-22T15:47:58.135314Z"
    },
    "papermill": {
     "duration": 0.487059,
     "end_time": "2023-05-22T15:47:58.139407",
     "exception": false,
     "start_time": "2023-05-22T15:47:57.652348",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ab33068/Documents/work/aml/challenge3/eurecom-aml-2023-challenge-3-baseline.ipynb Cell 20\u001b[0m in \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ab33068/Documents/work/aml/challenge3/eurecom-aml-2023-challenge-3-baseline.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# here we are obtaining the vocabulary from the training data minus validation data\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ab33068/Documents/work/aml/challenge3/eurecom-aml-2023-challenge-3-baseline.ipynb#X24sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# you may want to change this to the full training data for the final submission\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ab33068/Documents/work/aml/challenge3/eurecom-aml-2023-challenge-3-baseline.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m X_train_counts \u001b[39m=\u001b[39m count_vect\u001b[39m.\u001b[39;49mfit_transform(\u001b[39mlist\u001b[39;49m(train_df[\u001b[39m'\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mvalues))\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ab33068/Documents/work/aml/challenge3/eurecom-aml-2023-challenge-3-baseline.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m X_val_counts \u001b[39m=\u001b[39m count_vect\u001b[39m.\u001b[39mtransform(\u001b[39mlist\u001b[39m(val_df[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues))\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ab33068/Documents/work/aml/challenge3/eurecom-aml-2023-challenge-3-baseline.ipynb#X24sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m X_test_counts \u001b[39m=\u001b[39m count_vect\u001b[39m.\u001b[39mtransform(\u001b[39mlist\u001b[39m(test_df[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues))\n",
      "File \u001b[0;32m~/anaconda3/envs/malis/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:1338\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1330\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1331\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1332\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1333\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1334\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1335\u001b[0m             )\n\u001b[1;32m   1336\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m-> 1338\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[1;32m   1340\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[1;32m   1341\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/malis/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:1209\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1207\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[1;32m   1208\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[0;32m-> 1209\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[1;32m   1210\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1211\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m~/anaconda3/envs/malis/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:113\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    111\u001b[0m     doc \u001b[39m=\u001b[39m preprocessor(doc)\n\u001b[1;32m    112\u001b[0m \u001b[39mif\u001b[39;00m tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m     doc \u001b[39m=\u001b[39m tokenizer(doc)\n\u001b[1;32m    114\u001b[0m \u001b[39mif\u001b[39;00m ngrams \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m     \u001b[39mif\u001b[39;00m stop_words \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;32m/home/ab33068/Documents/work/aml/challenge3/eurecom-aml-2023-challenge-3-baseline.ipynb Cell 20\u001b[0m in \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ab33068/Documents/work/aml/challenge3/eurecom-aml-2023-challenge-3-baseline.ipynb#X24sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mauto_tokenizer\u001b[39m(text):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ab33068/Documents/work/aml/challenge3/eurecom-aml-2023-challenge-3-baseline.ipynb#X24sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     bert_uncased_tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m'\u001b[39;49m\u001b[39mbert-base-uncased\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ab33068/Documents/work/aml/challenge3/eurecom-aml-2023-challenge-3-baseline.ipynb#X24sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39m# bert_cased_tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ab33068/Documents/work/aml/challenge3/eurecom-aml-2023-challenge-3-baseline.ipynb#X24sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m# roberta_base_tokenizer = AutoTokenizer.from_pretrained('roberta-base')\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ab33068/Documents/work/aml/challenge3/eurecom-aml-2023-challenge-3-baseline.ipynb#X24sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39m# roberta_large_tokenizer = AutoTokenizer.from_pretrained('roberta-large')\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ab33068/Documents/work/aml/challenge3/eurecom-aml-2023-challenge-3-baseline.ipynb#X24sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39m# gpt_tokenizer = AutoTokenizer.from_pretrained('gpt2', add_special_tokens=True, use_fast=True)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ab33068/Documents/work/aml/challenge3/eurecom-aml-2023-challenge-3-baseline.ipynb#X24sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     tokens \u001b[39m=\u001b[39m bert_uncased_tokenizer\u001b[39m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m~/anaconda3/envs/malis/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py:643\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    640\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer_class\u001b[39m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    642\u001b[0m \u001b[39m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[0;32m--> 643\u001b[0m tokenizer_config \u001b[39m=\u001b[39m get_tokenizer_config(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    644\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m tokenizer_config:\n\u001b[1;32m    645\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m tokenizer_config[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/malis/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py:487\u001b[0m, in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[39mLoads the tokenizer configuration from a pretrained model tokenizer configuration.\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[39mtokenizer_config = get_tokenizer_config(\"tokenizer-test\")\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[39m```\"\"\"\u001b[39;00m\n\u001b[1;32m    486\u001b[0m commit_hash \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 487\u001b[0m resolved_config_file \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m    488\u001b[0m     pretrained_model_name_or_path,\n\u001b[1;32m    489\u001b[0m     TOKENIZER_CONFIG_FILE,\n\u001b[1;32m    490\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    491\u001b[0m     force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    492\u001b[0m     resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    493\u001b[0m     proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    494\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    495\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    496\u001b[0m     local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    497\u001b[0m     subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m    498\u001b[0m     _raise_exceptions_for_missing_entries\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    499\u001b[0m     _raise_exceptions_for_connection_errors\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    500\u001b[0m     _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m    501\u001b[0m )\n\u001b[1;32m    502\u001b[0m \u001b[39mif\u001b[39;00m resolved_config_file \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    503\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/malis/lib/python3.8/site-packages/transformers/utils/hub.py:417\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    414\u001b[0m user_agent \u001b[39m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    415\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    416\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 417\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    418\u001b[0m         path_or_repo_id,\n\u001b[1;32m    419\u001b[0m         filename,\n\u001b[1;32m    420\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[1;32m    421\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[1;32m    422\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    423\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    424\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    425\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    426\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    427\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    428\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    429\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    430\u001b[0m     )\n\u001b[1;32m    432\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[1;32m    433\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    434\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m is not a local folder and is not a valid model identifier \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    435\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlisted on \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIf this is a private repository, make sure to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    436\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpass a token having permission to this repo with `use_auth_token` or log in with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    437\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`huggingface-cli login` and pass `use_auth_token=True`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    438\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/malis/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/malis/lib/python3.8/site-packages/huggingface_hub/file_download.py:1195\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1194\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m         metadata \u001b[39m=\u001b[39m get_hf_file_metadata(\n\u001b[1;32m   1196\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m   1197\u001b[0m             token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   1198\u001b[0m             proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1199\u001b[0m             timeout\u001b[39m=\u001b[39;49metag_timeout,\n\u001b[1;32m   1200\u001b[0m         )\n\u001b[1;32m   1201\u001b[0m     \u001b[39mexcept\u001b[39;00m EntryNotFoundError \u001b[39mas\u001b[39;00m http_error:\n\u001b[1;32m   1202\u001b[0m         \u001b[39m# Cache the non-existence of the file and raise\u001b[39;00m\n\u001b[1;32m   1203\u001b[0m         commit_hash \u001b[39m=\u001b[39m http_error\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(HUGGINGFACE_HEADER_X_REPO_COMMIT)\n",
      "File \u001b[0;32m~/anaconda3/envs/malis/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/malis/lib/python3.8/site-packages/huggingface_hub/file_download.py:1532\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout)\u001b[0m\n\u001b[1;32m   1529\u001b[0m headers[\u001b[39m\"\u001b[39m\u001b[39mAccept-Encoding\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39midentity\u001b[39m\u001b[39m\"\u001b[39m  \u001b[39m# prevent any compression => we want to know the real size of the file\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1532\u001b[0m r \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[1;32m   1533\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mHEAD\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1534\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m   1535\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m   1536\u001b[0m     allow_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1537\u001b[0m     follow_relative_redirects\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1538\u001b[0m     proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1539\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m   1540\u001b[0m )\n\u001b[1;32m   1541\u001b[0m hf_raise_for_status(r)\n\u001b[1;32m   1543\u001b[0m \u001b[39m# Return\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/malis/lib/python3.8/site-packages/huggingface_hub/file_download.py:407\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, timeout, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[39m# 2. Force relative redirection\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[39mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 407\u001b[0m     response \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[1;32m    408\u001b[0m         method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m    409\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    410\u001b[0m         max_retries\u001b[39m=\u001b[39;49mmax_retries,\n\u001b[1;32m    411\u001b[0m         base_wait_time\u001b[39m=\u001b[39;49mbase_wait_time,\n\u001b[1;32m    412\u001b[0m         max_wait_time\u001b[39m=\u001b[39;49mmax_wait_time,\n\u001b[1;32m    413\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    414\u001b[0m         follow_relative_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    415\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams,\n\u001b[1;32m    416\u001b[0m     )\n\u001b[1;32m    418\u001b[0m     \u001b[39m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    419\u001b[0m     \u001b[39m# This is useful in case of a renamed repository.\u001b[39;00m\n\u001b[1;32m    420\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m300\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mstatus_code \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m399\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/malis/lib/python3.8/site-packages/huggingface_hub/file_download.py:442\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, timeout, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n\u001b[1;32m    441\u001b[0m \u001b[39m# 3. Exponential backoff\u001b[39;00m\n\u001b[0;32m--> 442\u001b[0m \u001b[39mreturn\u001b[39;00m http_backoff(\n\u001b[1;32m    443\u001b[0m     method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m    444\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    445\u001b[0m     max_retries\u001b[39m=\u001b[39;49mmax_retries,\n\u001b[1;32m    446\u001b[0m     base_wait_time\u001b[39m=\u001b[39;49mbase_wait_time,\n\u001b[1;32m    447\u001b[0m     max_wait_time\u001b[39m=\u001b[39;49mmax_wait_time,\n\u001b[1;32m    448\u001b[0m     retry_on_exceptions\u001b[39m=\u001b[39;49m(ConnectTimeout, ProxyError),\n\u001b[1;32m    449\u001b[0m     retry_on_status_codes\u001b[39m=\u001b[39;49m(),\n\u001b[1;32m    450\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    451\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams,\n\u001b[1;32m    452\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/malis/lib/python3.8/site-packages/huggingface_hub/utils/_http.py:212\u001b[0m, in \u001b[0;36mhttp_backoff\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mseek(io_obj_initial_pos)\n\u001b[1;32m    211\u001b[0m \u001b[39m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m response \u001b[39m=\u001b[39m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    213\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m retry_on_status_codes:\n\u001b[1;32m    214\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/anaconda3/envs/malis/lib/python3.8/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/envs/malis/lib/python3.8/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    704\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/anaconda3/envs/malis/lib/python3.8/site-packages/requests/adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    488\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 489\u001b[0m         resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    490\u001b[0m             method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    491\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    492\u001b[0m             body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    493\u001b[0m             headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    494\u001b[0m             redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m             assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    496\u001b[0m             preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    497\u001b[0m             decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    498\u001b[0m             retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    499\u001b[0m             timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    500\u001b[0m         )\n\u001b[1;32m    502\u001b[0m     \u001b[39m# Send the request.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39mproxy_pool\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/malis/lib/python3.8/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    704\u001b[0m     conn,\n\u001b[1;32m    705\u001b[0m     method,\n\u001b[1;32m    706\u001b[0m     url,\n\u001b[1;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/malis/lib/python3.8/site-packages/urllib3/connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    444\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[1;32m    445\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    450\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    451\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/malis/lib/python3.8/site-packages/urllib3/connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n\u001b[1;32m    443\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    445\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/malis/lib/python3.8/http/client.py:1348\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1346\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1347\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1348\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1349\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m   1350\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/malis/lib/python3.8/http/client.py:316\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 316\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    317\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[1;32m    318\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/malis/lib/python3.8/http/client.py:277\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 277\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    278\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[1;32m    279\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/malis/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    670\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    671\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/malis/lib/python3.8/ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1237\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1238\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1239\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1240\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1241\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1242\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1243\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/anaconda3/envs/malis/lib/python3.8/ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1098\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1099\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1100\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1101\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# here we are obtaining the vocabulary from the training data minus validation data\n",
    "# you may want to change this to the full training data for the final submission\n",
    "X_train_counts = count_vect.fit_transform(list(train_df['text'].values))\n",
    "X_val_counts = count_vect.transform(list(val_df['text'].values))\n",
    "X_test_counts = count_vect.transform(list(test_df['text'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601bef13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-22T15:47:58.160502Z",
     "iopub.status.busy": "2023-05-22T15:47:58.159305Z",
     "iopub.status.idle": "2023-05-22T15:47:58.165652Z",
     "shell.execute_reply": "2023-05-22T15:47:58.164582Z"
    },
    "papermill": {
     "duration": 0.019339,
     "end_time": "2023-05-22T15:47:58.167921",
     "exception": false,
     "start_time": "2023-05-22T15:47:58.148582",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train feature shape: (22258, 2)\n",
      "Validation feature shape: (2474, 2)\n",
      "Test feature shape: (2748, 2)\n"
     ]
    }
   ],
   "source": [
    "print('Train feature shape:', X_train_counts.shape)\n",
    "print('Validation feature shape:', X_val_counts.shape)\n",
    "print('Test feature shape:', X_test_counts.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0fa6fde5",
   "metadata": {
    "papermill": {
     "duration": 0.009659,
     "end_time": "2023-05-22T15:47:58.187642",
     "exception": false,
     "start_time": "2023-05-22T15:47:58.177983",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The Bag-of-Words representation assigns a unique ID to each word that appears in the training data. 23239 unique words have been extracted. Each input data point (tweet) is then represented by a vector of the size of the vocabulary. Each of its elements are the counts of the respective word appearing in the tweet.\n",
    "\n",
    "Therefore, the features have a huge dimension! Storing the feature matrix directly would require (n_datapoints x vocabulary size) * 32 bits  \n",
    "  2 GB CPU/GPU RAM! Imagine we were not analyzing tweets (limited vocabulary) but Wikipedia! Or imagine we had a larger corpus of documents. Then we could not store the features!\n",
    "\n",
    "Instead, the Bag-of-Words features are usually stored using a sparse representation. Imagine this like a dictionary of ID-count tuples assigned to each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e790898",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-22T15:47:58.208782Z",
     "iopub.status.busy": "2023-05-22T15:47:58.208327Z",
     "iopub.status.idle": "2023-05-22T15:47:58.215726Z",
     "shell.execute_reply": "2023-05-22T15:47:58.214213Z"
    },
    "papermill": {
     "duration": 0.020285,
     "end_time": "2023-05-22T15:47:58.217887",
     "exception": false,
     "start_time": "2023-05-22T15:47:58.197602",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<22258x2 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 44516 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we quickly analyze the matrix of word counts:\n",
    "# Only 255125 of the 22258x23162 values => 0.049487% are non-zero.\n",
    "# The sparse encoding only needs to store these.\n",
    "X_train_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c86674",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-22T15:47:58.241105Z",
     "iopub.status.busy": "2023-05-22T15:47:58.240325Z",
     "iopub.status.idle": "2023-05-22T15:47:58.248887Z",
     "shell.execute_reply": "2023-05-22T15:47:58.247445Z"
    },
    "papermill": {
     "duration": 0.024178,
     "end_time": "2023-05-22T15:47:58.251391",
     "exception": false,
     "start_time": "2023-05-22T15:47:58.227213",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1],\n",
       "       [1, 1]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Yet, we can ask to convert a part of the matrix into the traditional dense format.\n",
    "# It's quite challenging to find any non-zeros here!\n",
    "X_train_counts[:10,:10].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfed126",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-22T15:47:58.276171Z",
     "iopub.status.busy": "2023-05-22T15:47:58.275386Z",
     "iopub.status.idle": "2023-05-22T15:47:58.281975Z",
     "shell.execute_reply": "2023-05-22T15:47:58.280936Z"
    },
    "papermill": {
     "duration": 0.022665,
     "end_time": "2023-05-22T15:47:58.284408",
     "exception": false,
     "start_time": "2023-05-22T15:47:58.261743",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The other way around is easier. We can ask to find the ID (index) of a specific word.\n",
    "count_vect.vocabulary_.get('sleep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35054083",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-22T15:47:58.305616Z",
     "iopub.status.busy": "2023-05-22T15:47:58.305083Z",
     "iopub.status.idle": "2023-05-22T15:47:58.312632Z",
     "shell.execute_reply": "2023-05-22T15:47:58.311448Z"
    },
    "papermill": {
     "duration": 0.020809,
     "end_time": "2023-05-22T15:47:58.314800",
     "exception": false,
     "start_time": "2023-05-22T15:47:58.293991",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# So the first tweet should have a one at this position:\n",
    "print('Tweet:\\n', train_df.iloc[0]['text'])\n",
    "print('Number of times the word \"sleep\" appeared:\\n', X_train_counts[0, 18618])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "189ac5bb",
   "metadata": {
    "papermill": {
     "duration": 0.009336,
     "end_time": "2023-05-22T15:47:58.334232",
     "exception": false,
     "start_time": "2023-05-22T15:47:58.324896",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training a simple classifier\n",
    "\n",
    "We are training a naive Bayes classifier on the Bag-of-Words features of the training data:\n",
    "\n",
    "https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html\n",
    "\n",
    "It is already built into the sklearn library:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\n",
    "\n",
    "Keep in mind that not only storing the features is challenging but also processing them. A simple SVM may be quite slow on such high-dimensional features. Naive Bayes works well with Bag-of-Words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e86e7b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-22T15:47:58.356522Z",
     "iopub.status.busy": "2023-05-22T15:47:58.355741Z",
     "iopub.status.idle": "2023-05-22T15:47:58.377310Z",
     "shell.execute_reply": "2023-05-22T15:47:58.375769Z"
    },
    "papermill": {
     "duration": 0.03574,
     "end_time": "2023-05-22T15:47:58.379558",
     "exception": false,
     "start_time": "2023-05-22T15:47:58.343818",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.15 ms, sys: 159 s, total: 7.31 ms\n",
      "Wall time: 6.48 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = MultinomialNB().fit(X_train_counts, train_df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feaac84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-22T15:47:58.401904Z",
     "iopub.status.busy": "2023-05-22T15:47:58.401127Z",
     "iopub.status.idle": "2023-05-22T15:47:58.408299Z",
     "shell.execute_reply": "2023-05-22T15:47:58.407517Z"
    },
    "papermill": {
     "duration": 0.020647,
     "end_time": "2023-05-22T15:47:58.410499",
     "exception": false,
     "start_time": "2023-05-22T15:47:58.389852",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_predictions_nb = clf.predict(X_val_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e02a8ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-22T15:47:58.433351Z",
     "iopub.status.busy": "2023-05-22T15:47:58.432959Z",
     "iopub.status.idle": "2023-05-22T15:47:58.439950Z",
     "shell.execute_reply": "2023-05-22T15:47:58.438222Z"
    },
    "papermill": {
     "duration": 0.020954,
     "end_time": "2023-05-22T15:47:58.442135",
     "exception": false,
     "start_time": "2023-05-22T15:47:58.421181",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of our multinomial Naive Bayes classifier is: 40.30%\n"
     ]
    }
   ],
   "source": [
    "accuracy = (val_predictions_nb == val_df['target'].values).mean()\n",
    "print('The accuracy of our multinomial Naive Bayes classifier is: {:.2f}%'.format(accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16937191",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-22T15:47:58.464779Z",
     "iopub.status.busy": "2023-05-22T15:47:58.464309Z",
     "iopub.status.idle": "2023-05-22T15:47:58.475264Z",
     "shell.execute_reply": "2023-05-22T15:47:58.473609Z"
    },
    "papermill": {
     "duration": 0.025117,
     "end_time": "2023-05-22T15:47:58.477728",
     "exception": false,
     "start_time": "2023-05-22T15:47:58.452611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fbeta score is: 0.19149140497455105\n"
     ]
    }
   ],
   "source": [
    "fbeta = fbeta_score(val_df['target'].values, val_predictions_nb, average='macro', beta=1.0)\n",
    "print('The fbeta score is:', fbeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a493e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-22T15:47:58.502341Z",
     "iopub.status.busy": "2023-05-22T15:47:58.500783Z",
     "iopub.status.idle": "2023-05-22T15:47:58.980825Z",
     "shell.execute_reply": "2023-05-22T15:47:58.979671Z"
    },
    "papermill": {
     "duration": 0.496208,
     "end_time": "2023-05-22T15:47:58.984220",
     "exception": false,
     "start_time": "2023-05-22T15:47:58.488012",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating a submission\n",
    "\n",
    "X_train_counts = count_vect.fit_transform(list(train_df['text'].values) + list(val_df['text'].values))\n",
    "X_test_counts = count_vect.transform(list(test_df['text'].values))\n",
    "\n",
    "clf = MultinomialNB().fit(X_train_counts, np.hstack([train_df['target'].values, val_df['target'].values]))\n",
    "test_predictions_nb = clf.predict(X_test_counts)\n",
    "\n",
    "submission_df = pd.DataFrame()\n",
    "submission_df['textID'] = test_df['textID']\n",
    "submission_df['sentiment'] = test_predictions_nb\n",
    "submission_df.to_csv('TA_baseline_NB.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "df01cbfe",
   "metadata": {
    "papermill": {
     "duration": 0.011562,
     "end_time": "2023-05-22T15:47:59.005787",
     "exception": false,
     "start_time": "2023-05-22T15:47:58.994225",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## How good is this score?\n",
    "\n",
    "Early approaches in NLP used rule-based classifiers for sentiment analysis. A popular baseline is VADER which was published in 2014:\n",
    "\n",
    "https://www.aaai.org/ocs/index.php/ICWSM/ICWSM14/paper/viewPaper/8109\n",
    "\n",
    "VADER does not use any machine learning but is purely handcrafted by humans. It uses text preprocessing and lexica to determine the sentiment of a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c31cc37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-22T15:47:59.027479Z",
     "iopub.status.busy": "2023-05-22T15:47:59.027073Z",
     "iopub.status.idle": "2023-05-22T15:47:59.215950Z",
     "shell.execute_reply": "2023-05-22T15:47:59.214614Z"
    },
    "papermill": {
     "duration": 0.202593,
     "end_time": "2023-05-22T15:47:59.218487",
     "exception": false,
     "start_time": "2023-05-22T15:47:59.015894",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/ab33068/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f0b93d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-22T15:47:59.241740Z",
     "iopub.status.busy": "2023-05-22T15:47:59.241347Z",
     "iopub.status.idle": "2023-05-22T15:47:59.260676Z",
     "shell.execute_reply": "2023-05-22T15:47:59.259476Z"
    },
    "papermill": {
     "duration": 0.035011,
     "end_time": "2023-05-22T15:47:59.263978",
     "exception": false,
     "start_time": "2023-05-22T15:47:59.228967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d77c810",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-22T15:47:59.287530Z",
     "iopub.status.busy": "2023-05-22T15:47:59.287084Z",
     "iopub.status.idle": "2023-05-22T15:47:59.294787Z",
     "shell.execute_reply": "2023-05-22T15:47:59.293837Z"
    },
    "papermill": {
     "duration": 0.022,
     "end_time": "2023-05-22T15:47:59.297174",
     "exception": false,
     "start_time": "2023-05-22T15:47:59.275174",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " couple days?! Sheeeeit. Wish I were there. Have something at Cafe Claude for me.\n",
      "{'neg': 0.0, 'neu': 0.8, 'pos': 0.2, 'compound': 0.4574}\n",
      "She didn`t make the challenge\n",
      "{'neg': 0.0, 'neu': 0.755, 'pos': 0.245, 'compound': 0.0772}\n",
      "@_laertesgirl Ohh of course. She did stop once though, end of Dream. John Woodvine & Zoe Thorne, only ones not signed my programme\n",
      "{'neg': 0.091, 'neu': 0.826, 'pos': 0.083, 'compound': -0.0516}\n",
      "happy mother`s day mom. love you always\n",
      "{'neg': 0.0, 'neu': 0.388, 'pos': 0.612, 'compound': 0.836}\n",
      "I`m at my saddest right now. I lost my mobile phone earphones. Waz feels a little incomplete and I feel the same\n",
      "{'neg': 0.27, 'neu': 0.73, 'pos': 0.0, 'compound': -0.743}\n"
     ]
    }
   ],
   "source": [
    "# We show a few prediction examples:\n",
    "for doc in val_df['text'].iloc[:5].values:\n",
    "    print(doc)\n",
    "    print(sid.polarity_scores(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478bc6b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-22T15:47:59.320905Z",
     "iopub.status.busy": "2023-05-22T15:47:59.320501Z",
     "iopub.status.idle": "2023-05-22T15:47:59.326814Z",
     "shell.execute_reply": "2023-05-22T15:47:59.325868Z"
    },
    "papermill": {
     "duration": 0.02075,
     "end_time": "2023-05-22T15:47:59.329099",
     "exception": false,
     "start_time": "2023-05-22T15:47:59.308349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def vader_predict(x):\n",
    "    prediction = sid.polarity_scores(x)\n",
    "    prediction_list = [\n",
    "        (1, prediction['pos']),\n",
    "        (-1, prediction['neg']),\n",
    "        (0, prediction['neu'])\n",
    "    ]\n",
    "    label = sorted(prediction_list, key=lambda x: x[1], reverse=True)[0][0]\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a58f284",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-22T15:47:59.352271Z",
     "iopub.status.busy": "2023-05-22T15:47:59.351933Z",
     "iopub.status.idle": "2023-05-22T15:47:59.872561Z",
     "shell.execute_reply": "2023-05-22T15:47:59.871050Z"
    },
    "papermill": {
     "duration": 0.536793,
     "end_time": "2023-05-22T15:47:59.876745",
     "exception": false,
     "start_time": "2023-05-22T15:47:59.339952",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions_vader = val_df['text'].apply(vader_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d3c297",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-22T15:47:59.909990Z",
     "iopub.status.busy": "2023-05-22T15:47:59.909533Z",
     "iopub.status.idle": "2023-05-22T15:47:59.916203Z",
     "shell.execute_reply": "2023-05-22T15:47:59.915414Z"
    },
    "papermill": {
     "duration": 0.026732,
     "end_time": "2023-05-22T15:47:59.919575",
     "exception": false,
     "start_time": "2023-05-22T15:47:59.892843",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of VADER is: 48.67%\n"
     ]
    }
   ],
   "source": [
    "accuracy = (predictions_vader == val_df['target'].values).mean()\n",
    "print('The accuracy of VADER is: {:.2f}%'.format(accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940e25c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-22T15:47:59.952441Z",
     "iopub.status.busy": "2023-05-22T15:47:59.952016Z",
     "iopub.status.idle": "2023-05-22T15:47:59.969255Z",
     "shell.execute_reply": "2023-05-22T15:47:59.963145Z"
    },
    "papermill": {
     "duration": 0.037133,
     "end_time": "2023-05-22T15:47:59.972045",
     "exception": false,
     "start_time": "2023-05-22T15:47:59.934912",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fbeta score is: 0.37428265742622274\n"
     ]
    }
   ],
   "source": [
    "fbeta = fbeta_score(val_df['target'].values, predictions_vader, average='macro', beta=1.0)\n",
    "print('The fbeta score is:', fbeta)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e1a25d7",
   "metadata": {
    "papermill": {
     "duration": 0.012881,
     "end_time": "2023-05-22T15:47:59.996514",
     "exception": false,
     "start_time": "2023-05-22T15:47:59.983633",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "VADER performs worse! That is a good sign that our classifier learned useful generalizations from the training data (better than standard handcrafted rules)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4e64845",
   "metadata": {
    "papermill": {
     "duration": 0.011512,
     "end_time": "2023-05-22T15:48:00.019748",
     "exception": false,
     "start_time": "2023-05-22T15:48:00.008236",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Where to go from here?\n",
    "\n",
    "We can improve our Machine Learning pipeline on multiple aspects:\n",
    "\n",
    "### Data analysis:\n",
    "How is the data distributed? Can we analyze our data to find patterns associated with the classes? Which kinds of words are useful, which aren't?\n",
    "\n",
    "### Feature extraction:\n",
    "Can we make our Bag-of-Words representation more compact or richer? There are many things you could try to implement. Here are some buzzwords: tokenization, stop words removal, lemmatization, n-gram extraction, ...\n",
    "A useful Python library to address these issues is: NLTK (https://www.nltk.org/)\n",
    "The sklearn CountVectorizer we used can be combined with NLTK preprocessing: https://scikit-learn.org/stable/modules/feature_extraction.html#customizing-the-vectorizer-classes\n",
    "Is there also a dense (as opposed to sparse) representation of documents (tweets in our case)? Buzzwords: word2vec, gloVe\n",
    "The state-of-the-art: ... are neural network language models, so-called Transformers. There are pretrained models available. If you feel comfortable with neural networks, fine-tuning and GPUs, have a look here: https://huggingface.co/transformers/\n",
    "\n",
    "In general, we also recommend spaCy as a convenient Python library that covers most of the above features at once and may be a great resource to start with: https://spacy.io/\n",
    "\n",
    "### Model selection:\n",
    "The model of choice highly depends on the previously extracted features. Depending on whether you obtain a sparse or dense feature representation, you have to choose an appropriate model!\n",
    "\n",
    "### Model evaluation:\n",
    "Make sure to select potential model hyperparameters using cross-validation or similar. Our evaluation metric of choice is the F1-score:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html#sklearn.metrics.fbeta_score\n",
    "\n",
    "We choose beta=1 and average=macro\n",
    "\n",
    "### Extension idea 1:\n",
    "Apart from classifying the sentiment of tweets, we can also try to determine which words are the reason for the classifier to determine the classification. Ground-truth labels for these words are contained in our training data. The evaluation will not take place on the Kaggle platform. You need to do it yourself. Use the Jaccard coefficient to evaluate the overlap between the selected words and the ground truth:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html#jaccard-similarity-coefficient-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372d0824",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-22T15:48:00.047669Z",
     "iopub.status.busy": "2023-05-22T15:48:00.047016Z",
     "iopub.status.idle": "2023-05-22T15:48:00.061061Z",
     "shell.execute_reply": "2023-05-22T15:48:00.060101Z"
    },
    "papermill": {
     "duration": 0.030136,
     "end_time": "2023-05-22T15:48:00.064047",
     "exception": false,
     "start_time": "2023-05-22T15:48:00.033911",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>772</th>\n",
       "      <td>had a horrible sleep + in a rather bad mood</td>\n",
       "      <td>in a rather bad mood</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11840</th>\n",
       "      <td>No but this is our poor week</td>\n",
       "      <td>No but this is our poor week</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16482</th>\n",
       "      <td>_Blogs Congratulations lovely Japanese Childre...</td>\n",
       "      <td>Congratulations lo</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10802</th>\n",
       "      <td>?Happy Birthday to u? Happy Birthday to u? Ha...</td>\n",
       "      <td>?Happy</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13679</th>\n",
       "      <td>i can`t go tonight   *Cait*</td>\n",
       "      <td>i can`t go tonight   *Cait*</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "772          had a horrible sleep + in a rather bad mood   \n",
       "11840                       No but this is our poor week   \n",
       "16482  _Blogs Congratulations lovely Japanese Childre...   \n",
       "10802   ?Happy Birthday to u? Happy Birthday to u? Ha...   \n",
       "13679                        i can`t go tonight   *Cait*   \n",
       "\n",
       "                      selected_text sentiment  \n",
       "772            in a rather bad mood  negative  \n",
       "11840  No but this is our poor week  negative  \n",
       "16482            Congratulations lo  positive  \n",
       "10802                        ?Happy  positive  \n",
       "13679   i can`t go tonight   *Cait*   neutral  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selected_text shows the words selected from text to lead to the classification stored in sentiment\n",
    "train_df[['text', 'selected_text', 'sentiment']].iloc[:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "77a2aa08",
   "metadata": {
    "papermill": {
     "duration": 0.01657,
     "end_time": "2023-05-22T15:48:00.097195",
     "exception": false,
     "start_time": "2023-05-22T15:48:00.080625",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Extension idea 2:\n",
    "\n",
    "You may want to give it a try to Kaggle's brand new feature called models!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 17.636291,
   "end_time": "2023-05-22T15:48:01.235515",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-05-22T15:47:43.599224",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
